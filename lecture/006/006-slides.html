<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Lecture .mono[006]</title>
    <meta charset="utf-8" />
    <meta name="author" content="Edward Rubin" />
    <meta name="date" content="2020-02-13" />
    <link href="006-slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="006-slides_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="006-slides_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <script src="006-slides_files/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="006-slides_files/jquery-1.12.4/jquery.min.js"></script>
    <link href="006-slides_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="006-slides_files/datatables-binding-0.8/datatables.js"></script>
    <link href="006-slides_files/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="006-slides_files/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="006-slides_files/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
    <link href="006-slides_files/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
    <script src="006-slides_files/crosstalk-1.0.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture .mono[006]
## Classification
### Edward Rubin
### 13 February 2020

---

exclude: true


---
layout: true
# Admin

---
class: inverse, middle
---
name: admin-today
## Material

.b[Last time] Shrinkage methods
- Ridge regression
- (The) lasso
- Elasticnet

.b[Today] Classification methods
- Introduction to classification
- Linear probability models
- Logistic regression

.note[Also:] Class will end today at 11:30am..super[.pink[â€ ]]

.footnote[
.pink[â€ ] ðŸŽ‰?
]



---
name: admin-soon

## Upcoming

.b[Readings] .note[Today] .it[ISL] Ch. 4

.b[Problem sets]
- .it[Shrinkage methods] Due today
- .it[Classification] Due next week
---
layout: true
# Classification

---
class: inverse, middle

---
name: intro
## Intro

.attn[Regression problems] seek to predict the number an outcome will takeâ€”integers (_e.g._, number of cats), reals (_e.g._, home/cat value), _etc._ .super[.pink[â€ ]]

.footnote[
.pink[â€ ] Maybe: Binary indicators...
]

--

.attn[Classification problems] instead seek to predict the category of an outcome

- .b[Binary outcomes]&lt;br&gt;success/failure; true/false; A or B; cat or .it[not cat];  _etc._

- .b[Multi-class outcomes]&lt;br&gt;yes, no, .it[or maybe]; colors; letters; type of cat;.super[.pink[â€ â€ ]] _etc._

.footnote[
.tran[â€  Maybe: Binary indicators...] .pink[â€ â€ ] It turns out, all of machine learning is about cats.
]

This type of outcome is often called a .it[qualitative] or .it[categorical] response.

---
name: examples
## Examples

For the past few weeks, we've been immersed in regression problems.

It's probably helpful to mention a few .hi[examples of classification problems].

--

- Using life/criminal history (and demographics?):&lt;br&gt;Can we predict whether a defendant is .b[granted bail]?

--

- Based upon a set of symptoms and observations:&lt;br&gt;Can we predict a patient's .b[medical condition](s)?

--

- From the pixels in an image:&lt;br&gt;Can we classify images as .b[bagel, puppy, or other]?

---
## Approach

One can imagine two.super[.pink[â€ ]] related .hi[approaches to classification]

.footnote[
.pink[â€ ] At least.
]


1. Predict .b[which category] the outcome will take.

1. Estimate the .b[probability of each category] for the outcome.

--

That said, the general approach will

- Take a set of training observations `\((x_1,y_1),\, (x_2,y_2),\,\ldots,\,(x_n,y_n)\)`
- Build a classifier `\(\hat{y}_o=\mathop{f}(x_o)\)`

all while balancing bias and variance..super[.pink[â€ â€ ]]

.footnote[
.tran[â€  At least.] .pink[â€ â€ ] Sound familiar?
]

---
layout: false
class: clear, middle

.qa[Q] If everything is so similar, can't we use regression methods?

.white[No]
---
class: clear, middle

.qa[Q] If everything is so similar, can't we use regression methods?

.qa[A] .it[Sometimes.]
--
 .it[Other times:] No.
--
 Plus you still need new tools.

---
layout: true
# Classification
## Why not regression?

---
name: no-regress

Regression methods are not made to deal with .b[multiple categories].

.ex[Ex.] Consider three medical diagnoses: .pink[stroke], .purple[overdose], and .orange[seizure].

Regression needs a numeric outcomeâ€”how should we code our categories?

--

.left-third[
.center.note[Option 1]
`$$Y=\begin{cases}
  \displaystyle 1 &amp; \text{if }\color{#e64173}{\text{ stroke}} \\
  \displaystyle 2 &amp; \text{if }\color{#6A5ACD}{\text{ overdose}} \\
  \displaystyle 3 &amp; \text{if }\color{#FFA500}{\text{ seizure}} \\
\end{cases}$$`
]

--

.left-third[
.center.note[Option 2]
`$$Y=\begin{cases}
  \displaystyle 1 &amp; \text{if }\color{#6A5ACD}{\text{ overdose}} \\
  \displaystyle 2 &amp; \text{if }\color{#e64173}{\text{ stroke}} \\
  \displaystyle 3 &amp; \text{if }\color{#FFA500}{\text{ seizure}} \\
\end{cases}$$`
]

--

.left-third[
.center.note[Option 3]
`$$Y=\begin{cases}
  \displaystyle 1 &amp; \text{if }\color{#FFA500}{\text{ seizure}} \\
  \displaystyle 2 &amp; \text{if }\color{#e64173}{\text{ stroke}} \\
  \displaystyle 3 &amp; \text{if }\color{#6A5ACD}{\text{ overdose}} \\
\end{cases}$$`
]

--

The categories' ordering is unclearâ€”let alone the actual valuation.
&lt;br&gt;
The choice of ordering and valuation can affect predictions. ðŸ˜¿

---

As we've seen, .b[binary outcomes] are simpler.

--

.ex[Ex] If we are only choosing between .pink[stroke] and .purple[overdose]

.left-wide[
.center.note[Option 1]
`$$Y=\begin{cases}
  \displaystyle 0 &amp; \text{if }\color{#e64173}{\text{ stroke}} \\
  \displaystyle 1 &amp; \text{if }\color{#6A5ACD}{\text{ overdose}} \\
\end{cases}$$`
]
.left-thin.center[&lt;br&gt;&lt;br&gt;.center[and]]
.left-wide[
.center.note[Option 2]
`$$Y=\begin{cases}
  \displaystyle 0 &amp; \text{if }\color{#6A5ACD}{\text{ overdose}} \\
  \displaystyle 1 &amp; \text{if }\color{#e64173}{\text{ stroke}} \\
\end{cases}$$`
]

.clear-up[
will provide the same results.
]

---
name: lpm

In these .b[binary outcome] cases, we .it[can] apply linear regression.

These models are called .attn[linear probability models] (LPMs).

The .b[predictions] from an LPM

1. estimate the conditional probability `\(y_i = 1\)`, _i.e._, `\(\mathop{\text{Pr}}\left(y_o = 1 \mid x_o\right)\)`

1. are not restricted to being between 0 and 1.super[.pink[â€ ]]

1. provide an orderingâ€”and a reasonable estimate of probability

.footnote[
.pink[â€ ] Some people get very worked up about this point.
]

--

.note[Other benefits:] Coefficients are easily interpreted + we know how OLS works.

---
layout: true
class: clear, middle

---

Let's consider an example: the `Default` dataset from `ISLR`

<div id="htmlwidget-31f6eeda866e13789732" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-31f6eeda866e13789732">{"x":{"filter":"none","data":[["No","No","Yes","No","No","Yes","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","No","Yes","No","No","No","No","No","No","No","No","No"],["No","Yes","No","No","No","No","No","No","No","No","Yes","No","No","Yes","Yes","No","Yes","No","Yes","No","No","No","No","Yes","Yes","Yes","Yes","Yes","No","Yes","No","No","No","Yes","No","No","No","Yes","No","No","Yes","No","No","Yes","No","No","Yes","No","No","No","No","No","No","No","Yes","No","Yes","No","Yes","No","No","No","No","No","Yes","No","Yes","No","No","No","No","No","No","No","Yes","No","No","No","No","No","Yes","Yes","No","No","Yes","No","Yes","No","No","No","No","Yes","Yes","No","No","No","Yes","No","No","Yes"],[939.098501835431,397.54248845196,1511.61095196469,301.319402807109,878.446109878267,1673.48634915562,310.130223883326,1272.05389130072,887.201436107651,230.868924844914,421.957264818512,1057.35087177407,891.403240954195,1216.58625032905,522.381180598609,1558.86107533,1464.39479015494,740.885186560821,0,1734.22804374841,1155.1750058879,728.356058470814,991.060968479028,640.819040858052,539.834793088502,141.026549625642,1207.05248263756,1288.44855995707,523.708897901411,1182.00008160582,1502.75774867973,131.633998250593,311.889288737823,1200.62447093622,770.431961924154,1177.2495984995,728.212549683954,1941.90292814168,0,674.204629620179,999.391696394237,670.422582334202,350.051851978195,764.627790674236,401.180757478873,751.354391933644,1360.86563270173,497.881905243542,634.696104527899,157.66047507126,0,605.220968306144,1092.99815937252,650.007041287637,492.657068547904,1322.96904167252,1743.79883736231,509.370892230809,1417.22549877778,1189.75213362091,1642.19231860975,1013.96374594514,678.018921587752,522.15345763795,127.663375125104,800.609332848504,2092.4585301161,182.370459994264,1507.24919460052,382.885062371549,768.37858481136,0,934.969703457723,621.312719186294,1384.73759704675,131.747075963338,772.732074090217,364.663051203468,612.960653042669,431.536055178104,847.056485301184,965.587370085486,932.056966292719,779.657300310476,1151.73331728929,368.086167001206,1458.89346152272,202.321208104165,1416.44476989919,615.704276560405,1807.68449070323,840.988909202537,677.552825772912,1073.16853283521,932.872998040885,1088.48809632531,1508.70177606082,1238.10517698086,309.517982126985,1169.42044435962],[45519.0189767343,22710.8657401321,53506.9449260287,51539.9523173009,29561.7830760682,49310.3329074721,37697.2201903282,44895.5933005043,41641.4535720123,32798.7825914845,21744.9742361818,39651.123507722,46611.7316362549,18140.6239934368,23440.0642225885,45255.9671130598,13968.5080055771,34196.067455362,9321.4637343151,37621.6331723075,40398.3993465472,32388.6157596306,37597.3230643757,19055.9165274273,13119.6858549153,11314.3917806231,24747.8899692653,21216.9625737112,48091.7445309658,18102.833748716,53129.7830394396,42028.0085987333,46116.1315257271,18973.9076818191,53398.3147279233,35419.6103097622,50403.5592512078,23467.126966094,33781.6563086993,46481.9526798699,23688.6489687946,53666.1923387936,48411.9866835366,26188.0694950508,39686.6759484968,27179.7616531196,22310.9295094468,55543.6246875475,32594.6922349326,42125.7336109977,24892.9156877399,21792.3215199717,37351.3396590005,41427.8773217528,10154.1562624726,51956.2918282558,17541.0028662017,39546.4726439018,16053.9742636275,50838.5172180485,24444.3121751467,49653.797125496,59416.7788647884,40717.1625219394,13942.2441190945,50324.5549267962,14514.7699586373,45507.9336965974,24057.5179445895,42620.5302295829,43325.9703901599,30360.5489438194,42325.7499854437,39372.0771909379,23083.6670866922,36947.7786180191,47161.2767003098,10239.9724854423,43392.887089339,46941.0589440984,13741.3270672054,16440.0996975194,45505.3072242837,40804.4757033746,23149.5474930787,51601.3608445693,21186.5629553544,37188.5693842123,33099.4968843458,39376.3946187014,42308.9544537047,15406.2074109607,20745.0153256246,51668.9608305471,61192.8971316666,38171.3720733572,25338.2646859801,37544.9421289734,35293.1935706219,19879.2481698692]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>default<\/th>\n      <th>student<\/th>\n      <th>balance<\/th>\n      <th>income<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":[2,3]}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 2, 2, 3, \",\", \".\");\nDTWidget.formatRound(this, row, data, 3, 0, 3, \",\", \".\");\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>

---
exclude: true



---

.hi-purple[The data:] The outcome, default, only takes two values (only 3.3% default).

&lt;img src="006-slides_files/figure-html/boxplot-default-balance-1.svg" style="display: block; margin: auto;" /&gt;

---


.hi-purple[The data:] The outcome, default, only takes two values (only 3.3% default).

&lt;img src="006-slides_files/figure-html/plot-default-points-1.svg" style="display: block; margin: auto;" /&gt;
---

.hi-pink[The linear probability model] struggles with prediction in this setting.

&lt;img src="006-slides_files/figure-html/plot-default-lpm-1.svg" style="display: block; margin: auto;" /&gt;

---

.hi-orange[Logistic regression] .it[appears] to offer an improvement.

&lt;img src="006-slides_files/figure-html/plot-default-logistic-1.svg" style="display: block; margin: auto;" /&gt;

---

So... what's logistic regression?

---
layout: true
# Logistic regression

---
class: inverse, middle

---
name: logistic-intro
## Intro

.attn[Logistic regression] .b[models the probability] that our outcome `\(Y\)` belongs to a .b[specific category] (often whichever category we think of as `TRUE`).

--

For example, we just saw a graph where
$$
`\begin{align}
  \mathop{\text{Pr}}\left(\text{Default} = \text{Yes} | \text{Balance}\right) = p(\text{Balance})
\end{align}`
$$
we are modeling the probability of `default` as a function of `balance`.

--

We use the .b[estimated probabilities] to .b[make predictions], _e.g._,
- if `\(p(\text{Balance})\geq 0.5\)`, we could predict "Yes" for Default
- to be conservative, we could predict "Yes" if `\(p(\text{Balance})\geq0.1\)`

---
name: logistic-logistic
## What's .it[logistic]?

We want to model probability as a function of the predictors `\(\left(\beta_0 + \beta_1 X\right)\)`.

.col-centered[
.hi-pink[Linear probability model]
&lt;br&gt;
.pink[linear] transform. of predictors

$$
`\begin{align}
  p(X) = \beta_0 + \beta_1 X
\end{align}`
$$
]

.col-centered[
.hi-orange[Logistic model]
&lt;br&gt;
.orange[logistic] transform. of predictors

$$
`\begin{align}
  p(X) = \dfrac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
\end{align}`
$$
]

.clear-up[
What does this .it[logistic function] `\(\left(\frac{e^x}{1+e^x}\right)\)` do?
]

1. ensures predictions are between 0 `\((x\rightarrow-\infty)\)` and 1 `\((x\rightarrow\infty)\)`

1. forces an S-shaped curved through the data (not linear)

---
## What's .it[logistic]?

With a little math, you can show
$$
`\begin{align}
  p(X) = \dfrac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}} \implies \color{#e64173}{\log \left( \dfrac{p(X)}{1-p(X)}\right)} = \color{#6A5ACD}{\beta_0 + \beta_1 X}
\end{align}`
$$

.note[New definition:] .hi-pink[log odds].super[.pink[â€ ]] on the RHS and .hi-purple[linear predictors] on the LHS.

.footnote[
.pink[â€ ] The "log odds" is sometimes called "logit".
]


--

1. .b[interpretation] of `\(\beta_j\)` is about .pink[log odds]â€”not probability

--

1. .b[changes in probability] due to `\(X\)` depend on level of `\(X\)`.super[.pink[â€ ]]

.footnote[
.tran[â€  The "log odds" is sometimes called "logit".] .pink[â€ â€ ] It's nonlinear!
]

---
name: logistic-estimation
## Estimation

Before we can start predicting, we need to estimate the `\(\beta_j\)`s.
$$
`\begin{align}
  p(X) = \dfrac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}} \implies \color{#e64173}{\log \left( \dfrac{p(X)}{1-p(X)}\right)} = \color{#6A5ACD}{\beta_0 + \beta_1 X}
\end{align}`
$$

We estimate logistic regression using .attn[maximum likelihood estimation].

--

.attn[Maximum likelihood estimation] (MLE) searches for the `\(\beta_j\)`s that make our data "most likely" given the model we've written.

---
name: logistic-mle
## Maximum likelihood

.attn[MLE] searches for the `\(\beta_j\)`s that make our data "most likely" using our model.

$$
`\begin{align}
  \color{#e64173}{\log \left( \dfrac{p(X)}{1-p(X)}\right)} = \color{#6A5ACD}{\beta_0 + \beta_1 X}
\end{align}`
$$

--

1. `\(\color{#6A5ACD}{\beta_j}\)` tells us how `\(x_j\)` affects the .pink[log odds]

--

1. odds `\(= \dfrac{p(X)}{1-p(X)}\)`.
--
 If `\(p(X) &gt; 0.5\)`, then odds `\(&gt;1\)` and .pink[log odds] `\(&gt; 0\)`.

--

So we want choose `\(\color{#6A5ACD}{\beta_j}\)` such that
- .pink[log odds] are above zero for observations where `\(y_i=1\)`
- .pink[log odds] even larger for areas of `\(x_j\)` where most `\(i\)`s have `\(y_i=1\)`

---
## Formally: The likelihood function

We estimate logistic regression by maximizing .attn[the likelihood function].super[.pink[â€ ]]

.footnote[
.pink[â€ ] Generally, we actually will maximize the .it[log] of the likelihood function.
]

$$
`\begin{align}
  \mathop{\ell}(\beta_0,\beta_1) = \prod_{i:y_i=1} \mathop{p}(x_i) \prod_{i:y_i=0} (1-\mathop{p}(x_i))
\end{align}`
$$

The likelihood function is maximized by
- making `\(p(x_i)\)` large for individuals with `\(y_i = 1\)`
- making `\(p(x_i)\)` small for individuals with `\(y_i = 0\)`

.it[Put simply:] Maximum likelihood maximizes a predictive performance, conditional on the model we have written down.

---
name: logistic-r
## In R

In R, you can run logistic regression using the `glm()` function.

--

.note[Aside:] Related to `lm`, `glm` stands for .it[generalized] (linear model).

--

"Generalized" essentially means that we're applying some transformation to `\(\beta_0 + \beta_1 X\)` like logistic regression applies the logistic function.

---
## In R

In R, you can run logistic regression using the `glm()` function.

.b[Key arguments] (very similar to `lm()`)

- specify a `formula`,.super[.pink[â€ ]] _e.g._, `y ~ .` or `y ~ x + I(x^2)`

- define `family = "binomial"` (so R knows to run logistic regression)

- give the function some `data`

.footnote[
.pink[â€ ] Notice that we're back in the world of needing to select a model...
]

--


```r
est_logistic = glm(
  i_default ~ balance,
* family = "binomial",
  data = default_df
)
```

---
layout: false
class: clear


```r
est_logistic %&gt;% summary()
```

```
#&gt; 
#&gt; Call:
#&gt; glm(formula = i_default ~ balance, family = "binomial", data = default_df)
#&gt; 
#&gt; Deviance Residuals: 
#&gt;     Min       1Q   Median       3Q      Max  
#&gt; -2.2697  -0.1465  -0.0589  -0.0221   3.7589  
#&gt; 
#&gt; Coefficients:
*#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    
*#&gt; (Intercept) -1.065e+01  3.612e-01  -29.49   &lt;2e-16 ***
*#&gt; balance      5.499e-03  2.204e-04   24.95   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 2920.6  on 9999  degrees of freedom
#&gt; Residual deviance: 1596.5  on 9998  degrees of freedom
#&gt; AIC: 1600.5
#&gt; 
#&gt; Number of Fisher Scoring iterations: 8
```

---
layout: true
# Logistic regression

---
name: logistic-prediction
## Estimates and predictions



Thus, our estimates are `\(\hat{\beta}_0 \approx -10.65\)` and `\(\hat{\beta}_1 \approx 0.0055\)`.

.note[Remember:] These coefficients are for the .b[log odds].

--

If we want .hi[to make predictions] for `\(y_i\)` (whether or not `\(i\)` defaults),
&lt;br&gt;then we first must .hi[estimate the probability] `\(\mathop{p}(\text{Balance})\)`
$$
`\begin{align}
  \hat{p}(\text{Balance}) = \dfrac{e^{\hat{\beta}_0 + \hat{\beta}_1 \text{Balance}}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 \text{Balance}}}
  \approx
  \dfrac{e^{-10.65 + 0.0055 \cdot \text{Balance}}}{1 + e^{-10.65 + 0.0055 \cdot \text{Balance}}}
\end{align}`
$$

--

- If `\(\text{Balance} = 0\)`, we then estimate `\(\mathop{\hat{p}} \approx 0.000024\)`
- If `\(\text{Balance} = 2,000\)`, we then estimate `\(\mathop{\hat{p}} \approx 0.586\)`
- If `\(\text{Balance} = 3,000\)`, we then estimate `\(\mathop{\hat{p}} \approx 0.997\)` .super[.pink[â€ ]]

.footnote[
.pink[â€ ] You get a sense of the nonlinearity of the predictors' effects.
]

---
layout: false
class: clear, middle

.hi-orange[Logistic regression]'s predictions of `\(\mathop{p}(\text{Balance})\)`

&lt;img src="006-slides_files/figure-html/plot-default-logistic-2-1.svg" style="display: block; margin: auto;" /&gt;

---
class: clear, middle

.note[Note:] Everything we've done so far extends to models with many predictors.

---
layout: true
# Logistic regression
## Prediction

.note[Old news:] You can use `predict()` to get predictions out of `glm` objects.

--

.b[New and important:] `predict()` produces multiple `type`.small[s] of predictions

1. `type = "response"` predicts .it[on the scale of the response variable]
&lt;br&gt;for logistic regression, this means .b[predicted probabilities] (0 to 1)

1. `type = "link"` predicts .it[on the scale of the linear predictors]
&lt;br&gt;for logistic regression, this means .b[predicted log odds] (-âˆž to âˆž)

.attn[Beware:] The default is `type = "link"`, which you may not want.

---

Putting it all together, we can get (estimated) probabilities `\(\hat{p}(X)\)`


```r
# Predictions on scale of response (outcome) variable
p_hat = predict(est_logistic, type = "response")
```

which we can use to make predictions on `\(y\)`


```r
# Predict '1' if p_hat is greater or equal to 0.5
y_hat = as.numeric(p_hat &gt;= 0.5)
```

---
layout: false
class: clear, middle

So how did we do?

---
layout: true
# Assessment

---
class: inverse, middle

---
name: how
## How did we do?

We guessed 97.25% of the observations correctly.

--

.qa[Q] 97.25% is pretty good, right?

--

.qa[A] It depends...
--
 Remember that 3.33% of the observations did not default.
--
&lt;br&gt;So we would get 96.67% right by guessing "No" for everyone..super[.pink[â€ ]]

.footnote[
.pink[â€ ] This idea is called the .it[null classifier].
]


--



We .it[did] guess 30.03% of the defaults
--
, which is clearer better than 0%.

--

.qa[Q] How can we more formally assess our model's performance?

--

.qa[A] All roads lead to the .attn[confusion matrix].

---
name: confusion
## The confusion matrix

The .attn[confusion matrix] is us a convenient way to display
&lt;br&gt;.hi-orange[correct] and .hi-purple[incorrect] predictions for each class of our outcome.



<table class="huxtable" style="border-collapse: collapse; margin-bottom: 2em; margin-top: 2em; width: 50%; margin-left: auto; margin-right: auto;  ">
<col><col><col><col><tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td colspan="2" style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Truth</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
</tr>
<tr>
<td rowspan="2" style="vertical-align: middle; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Prediction</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(255, 165, 0);">True Negative (TN)</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 0pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(106, 90, 205);">False Negative (FN)</span></td>
</tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(106, 90, 205);">False Positive (FP)</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(255, 165, 0);">True Positive (TP)</span></td>
</tr>
</table>


--

The .attn[accuracy] of a method is the share of .orange[correct] predictions, _i.e._,
.center[
.b[Accuracy] = (.hi-orange[TN] + .hi-orange[TP]) / (.hi-orange[TN] + .hi-orange[TP] + .hi-purple[FN] + .hi-purple[FP])
]

--

This matrix also helps display many other measures of assessment.

---
## The confusion matrix

.attn[Sensitivity:] the share of positive outcomes `\(Y=1\)` that we correctly predict.

.center[
.b[Sensitivity] = .hi-orange[TP] / (.hi-orange[TP] + .hi-purple[FN])
]

<table class="huxtable" style="border-collapse: collapse; margin-bottom: 2em; margin-top: 2em; width: 50%; margin-left: auto; margin-right: auto;  ">
<col><col><col><col><tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td colspan="2" style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Truth</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"><span style="color: rgb(106, 90, 205);">Yes</span></td>
</tr>
<tr>
<td rowspan="2" style="vertical-align: middle; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Prediction</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;">True Negative (TN)</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 0pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(106, 90, 205);">False Negative (FN)</span></td>
</tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;">False Positive (FP)</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(255, 165, 0);">True Positive (TP)</span></td>
</tr>
</table>


Sensitivity is also called .attn[recall] and the .attn[true-positive rate].

One minus sensitivity is the .attn[type-II error rate].
---
## The confusion matrix

.attn[Specificity:] the share of neg. outcomes `\((Y=0)\)` that we correctly predict.

.center[
.b[Specificity] = .hi-orange[TN] / (.hi-orange[TN] + .hi-purple[FP])
]

<table class="huxtable" style="border-collapse: collapse; margin-bottom: 2em; margin-top: 2em; width: 50%; margin-left: auto; margin-right: auto;  ">
<col><col><col><col><tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td colspan="2" style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Truth</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"><span style="color: rgb(106, 90, 205);">No</span></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
</tr>
<tr>
<td rowspan="2" style="vertical-align: middle; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Prediction</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(255, 165, 0);">True Negative (TN)</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 0pt; padding: 4pt 4pt 4pt 4pt;">False Negative (FN)</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(106, 90, 205);">False Positive (FP)</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt;">True Positive (TP)</td>
</tr>
</table>


One minus specificity is the .attn[false-positive rate] or .attn[type-I error rate].

---
## The confusion matrix

.attn[Precision:] the share of predicted positives `\((\hat{Y}=1)\)` that are correct.

.center[
.b[Precision] = .hi-orange[TP] / (.hi-orange[TP] + .hi-purple[FP])
]

<table class="huxtable" style="border-collapse: collapse; margin-bottom: 2em; margin-top: 2em; width: 50%; margin-left: auto; margin-right: auto;  ">
<col><col><col><col><tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td colspan="2" style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Truth</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"></td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: center; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Yes</td>
</tr>
<tr>
<td rowspan="2" style="vertical-align: middle; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">Prediction</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;">No</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;">True Negative (TN)</td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 1pt 0pt 0pt 0pt; padding: 4pt 4pt 4pt 4pt;">False Negative (FN)</td>
</tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt; font-weight: bold;"><span style="color: rgb(106, 90, 205);">Yes</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 1pt; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(106, 90, 205);">False Positive (FP)</span></td>
<td style="vertical-align: top; text-align: left; white-space: nowrap; padding: 4pt 4pt 4pt 4pt;"><span style="color: rgb(255, 165, 0);">True Positive (TP)</span></td>
</tr>
</table>


---
## Which assessment?

.qa[Q] So .it[which] criterion should we use?

--

.qa[A] You should use the .it[right] criterion for your context.

- Are true positives more valuable than true negatives?
--
&lt;br&gt;.note[Sensitivity] will be key.

--

- Do you want to have high confidence in predicted positives?
--
&lt;br&gt;.note[Precision] is your friend

--

- Are all errors equal?
--
&lt;br&gt;
.note[Accuracy] is perfect.

--

There's a lot more, _e.g._, the .attn[F.sub[1] score] combines precision and sensitivity.

---
name: cm-r
## Confusion in R

`confusionMatrix()` from `caret` calculates the confusion matrixâ€”and many other statistics.

- `data`: a `factor` vector of predictions (use `as.factor()` if needed)

- `reference`: a `factor` vector of true outcomes

--


```r
cm_logistic = confusionMatrix(
  # Our predictions
  data = y_hat %&gt;% as.factor(),
  # Truth
  reference = default_df$i_default %&gt;% as.factor()
)
```

---
layout: false
class: clear


```
#&gt; Confusion Matrix and Statistics 
#&gt;   
#&gt;            Reference 
#&gt;  Prediction    0    1 
#&gt;           0 9625  233 
#&gt;           1   42  100 
#&gt;                                             
#&gt;                 Accuracy : 0.9725           
#&gt;                   95% CI : (0.9691, 0.9756) 
#&gt;      No Information Rate : 0.9667           
#&gt;      P-Value [Acc &gt; NIR] : 0.0004973        
#&gt;                                             
#&gt;                    Kappa : 0.4093           
#&gt;                                             
#&gt;   Mcnemar's Test P-Value : &lt; 2.2e-16        
#&gt;                                             
#&gt;              Sensitivity : 0.9957           
#&gt;              Specificity : 0.3003           
#&gt;           Pos Pred Value : 0.9764           
#&gt;           Neg Pred Value : 0.7042           
#&gt;               Prevalence : 0.9667           
#&gt;           Detection Rate : 0.9625           
#&gt;     Detection Prevalence : 0.9858           
#&gt;        Balanced Accuracy : 0.6480
```

---
layout: true
# Assessment

---
## Thresholds

Your setting also dictates the "optimal" threshold that moves a prediction from one class (_e.g._, Default = No) to another class (Default = Yes).

The Bayes classifier suggests a probability threshold of 0.5.

The Bayes classifier can't be beat in terms of .note[accuracy], but if you have goals other than accuracy, you should consider other thresholds.

---
name: roc
layout: false
class: clear, middle

The .attn[ROC curve] depicts the .pink[error rates] for the two classes of outcomes.



&lt;img src="006-slides_files/figure-html/plot-roc-1.svg" style="display: block; margin: auto;" /&gt;

---
class: clear, middle

More on ROC curves next time.


---
name: sources
layout: false
# Sources

These notes draw upon

- [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) (*ISL*)&lt;br&gt;James, Witten, Hastie, and Tibshirani
---
# Table of contents

.col-left[
.smallest[
#### Admin
- [Today](#admin-today)
- [Upcoming](#admin-soon)

#### Classification
- [Introduction](#intro)
- [Introductory examples](#examples)
- [Why not linear regression](#no-regress)
- [Linear probability models](#lpm)

]
]
.col-right[
.smallest[

#### Logistic regression
- [Intro](#logistic-intro)
- [The logistic function](#logistic-logistic)
- [Estimation](#logistic-estimation)
- [Maximum likelihood](#logistic-mle)
- [In R](#logistic-r)
- [Prediction](#logistic-prediction)

#### Assessment
- [How did we do?](#how)
- [The confusion matrix](#confusion)
- [In R](#cm-r)
- [ROC curve](#roc)

#### Other
- [Sources/references](#sources)
]
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
